import pandas as pd
import numpy as np
import torch
from torch import nn
from linear_nn import Net # TODO: change name from Net to more descriptive name
from sklearn.preprocessing import MinMaxScaler
# from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# TODO: add features of stats by school team was eliminated by
# IDEA: weight training by single score and double score, ex. x/64 and /192
# IDEA: play with using target values rounds as 0-6, 1-7, and 64,32,16,8,4,2,1
# IDEA: create a different model for the first round, second round, etc.
# IDEA: try a model to predict the probability of a single game (logistic regression)

#%% codecell
# set random state
def set_random_seed(seed):
    """Set random seeds for libraries in use.

    Set initial random seed for each library in use which uses random seeds to help keep results as reproducible as possible.

    Parameters
    ----------
    seed : int
        Integer number acting as the random seed.

    Returns
    -------
    None
    """
    torch.manual_seed(seed)
    np.random.seed(seed)

# NOTE: important to normalize each fold seperately
def fold(data, target, batch_size, f):
    """Get one fold of data for k_fold_cross_validation.

    Split data into X_train, X_test, y_train, and y_test splits for a K Fold Cross Validation fold. Each fold is split into train and test data with test data being of size batch_size, and train data being the rest of the data. Folds are generated by taking data from index range [f * batch_size, (f * batch_size) + batch_size]. This ensures that each test fold is one complete year of March Madness data if batch_size = 64, which keeps each fold balanced across target values 0-6 wins.

    Parameters
    ----------
    data : type
        Training data to split into train and test (validation) splits.
    batch_size : type
        The size of the test split in each fold.
    f : int
        The fold number used to calculate the index range of the test data.

    Returns
    -------
    tuple (DataFrame, DataFrame, DataFrame, DataFrame, MinMaxScaler)
        Returns X_train, y_train, X_test, y_test splits and the scaler object fit on X_train which is needed to transform the holdout data before making predictions.
    """
    X = data.drop(target, axis=1)
    y = data[target]

    # create scaler
    scaler = MinMaxScaler()

    # training fold
    X_train = scaler.fit_transform(X.drop(X.index[f:f+batch_size], axis=0))
    X_train = torch.tensor(X_train, dtype=torch.float32, requires_grad=True)
    y_train = y.drop(X.index[f:f+batch_size], axis=0)
    y_train = torch.tensor(y_train.values, dtype=torch.float32, requires_grad=True)
    y_train = torch.unsqueeze(y_train,1)

    # validation fold
    # NOTE: transform X_validaiton on already fit scaler
    X_test = scaler.transform(X.iloc[f:f+batch_size])
    X_test = torch.tensor(X_test, dtype=torch.float32, requires_grad=False)
    y_test = y.iloc[f:f+batch_size]
    y_test = torch.tensor(y_test.values, dtype=torch.float32, requires_grad=False)
    y_test = torch.unsqueeze(y_test,1)

    return (X_train, y_train, X_test, y_test, scaler)

# TODO: ensemble model to reduce variance, look into rotating test year or doing cross validation in a smarter way
# IDEA: # TODO: change fold function to select randomly the correct distribution to get a mix of teams across different years for ability to train on more cross folds
def k_fold_cross_validation(data, holdout):
    # TODO: move to global params
    # number of folds, equal to the number of years of data excluding the holdout year for final predictions
    K = len(data.Year.unique())
    # NOTE: batch size of 64 for 64 teams each year in the tournament, gaurantees the correct distribution of rounds in each batch
    batch_size = 64
    learning_rate = 0.001
    epochs = 400
    device = "cpu"
    # TODO: RMSLE, bigger penalty on underestimating target than overestimating, more robust against outliers like cinderella teams
    loss_fn = nn.MSELoss()

    # create figs and axes for plotting trainnig and test loss
    rows, cols = 5, 6
    fig_multi, ax_multi = plt.subplots(rows, cols)
    fig_multi.set_size_inches(24,12)
    fig_single, ax_single = plt.subplots()
    fig_single.set_size_inches(12,12)

    # store each folds predictions
    fold_predictions = []

    # K Fold Cross Validation
    for F in range(K):
        # fold data into train and test splits with target variable WINS
        X, y, X_test, y_test, scaler = fold(data.drop(['School', 'Year'], axis=1), 'WINS', batch_size, F*batch_size)

        # store train and test errors of each train/test fold
        train_error = []
        test_error = []

        # NOTE NEED to initialize and train a new instance of the model and optimizer each fold to avoid leaking parameter information
        model = Net().to(device)
        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

        # TODO: early stopping
        # for each training/testing loop epoch
        for ep in range(epochs):
            train_rmse = train_loop(X, y, model, loss_fn, optimizer)
            predictions, test_rmse = test_loop(X_test, y_test, model, loss_fn)
            train_error.append(train_rmse.item())
            test_error.append(test_rmse.item())

        # plot each folds training and test error
        # ax.plot(range(epochs), train_error, label="Fold {} Training RMSE".format(F))
        ax_single.plot(range(epochs), test_error, label="Fold {} Test RMSE".format(F))
        ax_single.set_xlabel("Epochs")
        ax_single.set_ylabel("Wins")
        ax_single.legend()
        ax_single.set_title("K-Fold Test RMSE")
        plot_loss(ax_multi[F//cols][F%cols], train_error, test_error, F, epochs)
        fold_prediction = predict(model, holdout.copy(), scaler)
        fold_predictions.append(fold_prediction)

    fig_multi.tight_layout()
    plt.show()
    return (model, fold_predictions)

class RMSLELoss(nn.Module):
    def __init__(self):
        super().__init__()
        self.mse = nn.MSELoss()

    def forward(self, pred, actual):
        return torch.sqrt(self.mse(torch.log(pred + 1), torch.log(actual + 1)))

# TODO: clean up code so I'm not reusing too much from k fold
# retrain full model without k fold cross validation
def retrain(data):
    learning_rate = 0.001
    epochs = 400
    device = "cpu"
    loss_fn = nn.MSELoss()

    X_train = data.drop(['School', 'Year', 'WINS'], axis=1)
    y_train = data['WINS']

    scaler = MinMaxScaler()

    # training fold
    X = scaler.fit_transform(X_train)
    X = torch.tensor(X, dtype=torch.float32, requires_grad=True)
    y = torch.tensor(y_train.values, dtype=torch.float32, requires_grad=True)
    y = torch.unsqueeze(y,1)

    # NOTE NEED to initialize and train a new instance of the model and optimizer each fold to avoid leaking parameter information
    model = Net().to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

    # TODO: early stopping
    # for each training/testing loop epoch
    for ep in range(epochs):
        train_rmse = train_loop(X, y, model, loss_fn, optimizer)

    return model, scaler

#%% codecell
def train_loop(X, y, model, loss_fn, optimizer):
    model.train()
    logits = model(X)
    loss = torch.sqrt(loss_fn(logits, y)) # NOTE this is RMSE not MSE

    # backpropogation
    optimizer.zero_grad() # zero gradients so they don't add up
    loss.backward()
    optimizer.step()
    return loss

def test_loop(X, y, model, loss_fn):
    model.eval()
    with torch.no_grad():
        predictions = model(X)
        loss = torch.sqrt(loss_fn(predictions, y))
    return (predictions, loss)

# TODO save best model based on test data before prediction data
def predict(model, data, scaler):
    """Predict the number of March Madness wins for each team."""
    X = data.drop('School', axis=1)
    X = scaler.transform(X)
    X = torch.tensor(X, dtype=torch.float32, requires_grad=False)
    with torch.no_grad():
        model.eval()
        nn_predictions = model(X)
    nn_predictions = [p.item() for p in nn_predictions]
    prediction = data
    prediction.loc[:,'PRED'] = nn_predictions
    prediction = prediction.loc[:, ['School', 'PRED']]
    return prediction

def plot_loss(ax, train_error, validation_error, F, epochs):
    ax.plot(range(epochs), train_error, label="Training RMSE")
    ax.plot(range(epochs), validation_error, label="Test RMSE")
    ax.plot(range(epochs), np.ones(epochs), label="Target RMSE Threshhold")
    ax.set_xlabel("Epochs")
    ax.set_ylabel("Wins")
    # ax.legend()
    ax.set_title("Fold {} RMSE".format(F))

# TODO: goal is to get RMSE below 1 so the average erros is less than 1 win off
def main(target_year):
    set_random_seed(target_year)
    # import data
    df = pd.read_csv(r"Data/Clean/clean_aggregate.csv")

    features = ['SRS', 'Seed', 'AP_Mean', 'Pre', 'W', 'SRS_OPP', 'AW%', 'AG%', 'Guard_RSCI Top 100_Mean', 'PTS_per_Surgical_POS', 'MV_Mean', 'PIR', 'Forward_RSCI Top 100_Mean', 'FG%_MW_Pruned', 'Height_Mean_Pruned', 'Pos_MW_Pruned', 'TOV%', 'TRB%', 'FTr']

    # include school year and target variable to data
    features.extend(['School', 'Year', 'WINS'])

    # hold out target year data for final prediction
    data = df.loc[df['Year'] != target_year, features].copy()
    holdout = df.loc[df['Year'] == target_year, features].copy()

    # remove year and target variable from holdout data
    holdout = holdout.drop(['Year', 'WINS'], axis=1)

    # K-Fold Cross Validation
    model, fold_predictions = k_fold_cross_validation(data, holdout)

    # average the predictions from each folds model
    mean_predictions = pd.concat(fold_predictions).groupby('School').mean().sort_values(by='PRED', ascending=False)

    # retrained model predictions on full data set
    retrained_model, scaler = retrain(data)

    pred = predict(retrained_model, holdout, scaler)

    return pred, mean_predictions

# pred = main(2024)
pred, mean_pred = main(2024)
mean_pred['PRED'][:50]
pred.sort_values(by='PRED', ascending=False)[:50]
pred.loc[(pred['School'] == "Colorado State") | (pred['School'] == "Texas"), ['School','PRED']]

# NOTE: model 1.0 (kenpom 2008-2024) on 2024 picks: 41 total correct picks (24,11,3,1,1,1) 114 pts
# NOTE: model 1.1 (bs4 1997-2024) minus 1-2 features on 2024 picks and adjusted training length: 43 correct picks (21,12,5,2,2,1) 145 pts
